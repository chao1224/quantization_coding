{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quantization Coding\n",
    "\n",
    "Given a matrix $X$ of any shape, the goal is to get a appriximatly good encoder and decoder, such that\n",
    "\n",
    "$$X \\overset{Encoder}{\\longrightarrow\\longrightarrow\\longrightarrow} code \\overset{Decoder}{\\longrightarrow\\longrightarrow\\longrightarrow} X'$$\n",
    "\n",
    "The motivation of quantization is to compress float data (like model gradients) in a lossless way.\n",
    "\n",
    "+ Suppose each value in the array $x \\in X$ is a 8B(64 bit) float\n",
    "+ Want to quantize $X$ with $s$-bit\n",
    "+ $x = sign(x) \\times \\xi(x) \\times \\| X \\|_2^2$, where $\\xi(x)$ is\n",
    "\n",
    "    + Do normalization based on the 2-norm, $x' = \\frac{x}{\\|X\\|_2^2}$\n",
    "    \n",
    "    + Make $2^s$ intervals, corresponding to each value from $[0, 2^s-1]$.\n",
    "    \n",
    "    + For each $x'$, find the interval $[l, l+1]$ where it lies.\n",
    "    \n",
    "      $0 \\le l \\le x' \\le l+1 \\le 2^s -1$\n",
    "    \n",
    "    + Random sample a value in the corersponding interval, $v \\in [l, l+1]$\n",
    "    \n",
    "    + $\\begin{eqnarray}\n",
    "       \\xi(x) =\n",
    "       \\begin{cases}\n",
    "       l, & l \\le v \\le x'\\\\\n",
    "       l+1, & x' \\le v \\le l+1\n",
    "       \\end{cases}\n",
    "      \\end{eqnarray}$\n",
    "+ Therefore compress each float $x$ with 2-bit sign value and $s$-bit $\\xi$. Each 64-bit variable can include $64/(2 + s)$ float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(v, **kwargs):\n",
    "    if isinstance(v, (torch.Tensor, torch.cuda.FloatTensor)):\n",
    "        w = v.cpu().numpy().flat[:]\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        w = v.flat[:]\n",
    "    else:\n",
    "        raise ValueError(\"Object passed to encode not ndarray or torch.Tensor\")\n",
    "\n",
    "    norm = LA.norm(v)\n",
    "\n",
    "    quantization_level = kwargs['quantization_level']\n",
    "    s = (1 << quantization_level) - 1\n",
    "    shape = v.shape\n",
    "    num_int_each_64_bits = 64 / (2 + quantization_level)\n",
    "    num_section = num_int_each_64_bits\n",
    "    len_each_section = (w.shape[0] + num_section - 1) / num_section\n",
    "    w = np.pad(w, (0, len_each_section*num_section - w.shape[0]), mode='constant')\n",
    "\n",
    "    sign_array = np.sign(w)\n",
    "    sign_array += 1\n",
    "    sign_array = sign_array.astype('uint64')\n",
    "    normalization_array = np.abs(w) / norm * s\n",
    "\n",
    "    truncated_array = normalization_array.astype(int)\n",
    "    prob_array =  normalization_array - truncated_array\n",
    "    dice_array = np.random.rand(len(prob_array))\n",
    "    xi_array = truncated_array + (dice_array > prob_array)\n",
    "    xi_array = xi_array.astype('uint64')\n",
    "    \n",
    "    old_sign_array = sign_array\n",
    "    old_xi_array = xi_array\n",
    "    \n",
    "    xi_array = xi_array.reshape((num_section, len_each_section))\n",
    "    sign_array = sign_array.reshape((num_section, len_each_section))\n",
    "    \n",
    "    neo_array = np.zeros(len_each_section)\n",
    "    neo_array = neo_array.astype('uint64')\n",
    "\n",
    "    for i in range(num_int_each_64_bits):\n",
    "        xi = xi_array[i]\n",
    "        sign = sign_array[i]\n",
    "        neo_array <<= (2 + quantization_level)\n",
    "        neo_array = neo_array | (sign << quantization_level | xi)\n",
    "\n",
    "    code = {'neo': neo_array, 'norm': norm, 'quantization_level': quantization_level,\n",
    "            'len_each_section': len_each_section, 'num_int_each_64_bits': num_int_each_64_bits,\n",
    "            'shape': shape}\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(code, cuda=False, implementation='numpy', codes=[], **kwargs):\n",
    "    if implementation == 'numpy':\n",
    "        norm = code['norm']\n",
    "        quantization_level = code['quantization_level']\n",
    "        s = (1 << quantization_level) - 1\n",
    "            \n",
    "        real_size = np.prod(code['shape'])\n",
    "        \n",
    "        neo_array = code['neo'].astype('uint64')\n",
    "        num_int_each_64_bits = code['num_int_each_64_bits']\n",
    "        num_section = num_int_each_64_bits\n",
    "        len_each_section = code['len_each_section']\n",
    "        xi_array = np.ones((num_section, len_each_section))\n",
    "        sign_array = np.ones((num_section, len_each_section))\n",
    "        mask_for_xi = (1 << quantization_level) - 1\n",
    "        mask_for_sign = 3 << quantization_level\n",
    "        for i in range(num_int_each_64_bits)[::-1]:\n",
    "            sign_array[i] = (neo_array & mask_for_sign) >> quantization_level\n",
    "            xi_array[i] = neo_array & mask_for_xi\n",
    "            neo_array >>= (2 + quantization_level)\n",
    "        \n",
    "        xi_array = xi_array.reshape(-1).astype('uint64')\n",
    "        sign_array = sign_array.reshape(-1).astype('int8')\n",
    "        sign_array -= 1\n",
    "        v = sign_array * xi_array * norm / s\n",
    "        \n",
    "        v = v[:real_size]\n",
    "        v = v.reshape(code['shape'])\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Whoops, implementation')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each size: 8, total size: 160\n",
      "\n",
      "Original array:\n",
      "[ 0.95723658  0.1136488   0.14324333  0.58009869  0.45105075  0.41803557\n",
      "  0.7058255   0.53538994  0.20890947  0.61961766  0.65952282  0.05665859\n",
      "  0.43925004  0.52186763  0.73093325  0.77428901  0.61678737  0.61952965\n",
      "  0.16519608  0.9223033 ]\n"
     ]
    }
   ],
   "source": [
    "test_a = np.random.rand(20, 1)\n",
    "\n",
    "print 'each size: {}, total size: {}'.format(test_a[0].nbytes, test_a.nbytes)\n",
    "print\n",
    "print 'Original array:'\n",
    "print test_a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization level: 1\n",
      "[[ 0.        ]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 0.        ]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 0.        ]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]\n",
      " [ 2.56052405]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 2\n",
      "[[ 1.70701604]\n",
      " [ 0.85350802]\n",
      " [ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 1.70701604]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 3\n",
      "[[ 0.7315783 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.36578915]\n",
      " [ 0.7315783 ]\n",
      " [ 0.7315783 ]\n",
      " [ 0.36578915]\n",
      " [ 0.7315783 ]\n",
      " [ 0.        ]\n",
      " [ 0.36578915]\n",
      " [ 0.36578915]\n",
      " [ 0.36578915]\n",
      " [ 0.7315783 ]\n",
      " [ 0.36578915]\n",
      " [ 0.36578915]\n",
      " [ 1.09736745]\n",
      " [ 0.36578915]\n",
      " [ 0.36578915]\n",
      " [ 0.36578915]\n",
      " [ 0.7315783 ]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 4\n",
      "[[ 0.85350802]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.68280641]\n",
      " [ 0.34140321]\n",
      " [ 0.51210481]\n",
      " [ 0.85350802]\n",
      " [ 0.51210481]\n",
      " [ 0.1707016 ]\n",
      " [ 0.51210481]\n",
      " [ 0.51210481]\n",
      " [ 0.1707016 ]\n",
      " [ 0.51210481]\n",
      " [ 0.68280641]\n",
      " [ 0.85350802]\n",
      " [ 0.85350802]\n",
      " [ 0.68280641]\n",
      " [ 0.68280641]\n",
      " [ 0.        ]\n",
      " [ 0.85350802]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 5\n",
      "[[ 0.9911706 ]\n",
      " [ 0.08259755]\n",
      " [ 0.08259755]\n",
      " [ 0.6607804 ]\n",
      " [ 0.41298775]\n",
      " [ 0.41298775]\n",
      " [ 0.6607804 ]\n",
      " [ 0.57818285]\n",
      " [ 0.1651951 ]\n",
      " [ 0.6607804 ]\n",
      " [ 0.57818285]\n",
      " [ 0.        ]\n",
      " [ 0.4955853 ]\n",
      " [ 0.57818285]\n",
      " [ 0.6607804 ]\n",
      " [ 0.8259755 ]\n",
      " [ 0.6607804 ]\n",
      " [ 0.6607804 ]\n",
      " [ 0.24779265]\n",
      " [ 0.9911706 ]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 6\n",
      "[[ 0.9347945 ]\n",
      " [ 0.08128648]\n",
      " [ 0.16257296]\n",
      " [ 0.60964858]\n",
      " [ 0.48771887]\n",
      " [ 0.40643239]\n",
      " [ 0.7315783 ]\n",
      " [ 0.56900535]\n",
      " [ 0.24385943]\n",
      " [ 0.65029182]\n",
      " [ 0.69093506]\n",
      " [ 0.08128648]\n",
      " [ 0.44707563]\n",
      " [ 0.48771887]\n",
      " [ 0.69093506]\n",
      " [ 0.81286478]\n",
      " [ 0.65029182]\n",
      " [ 0.65029182]\n",
      " [ 0.20321619]\n",
      " [ 0.9347945 ]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 7\n",
      "[[ 0.96775712]\n",
      " [ 0.10080803]\n",
      " [ 0.16129285]\n",
      " [ 0.56452499]\n",
      " [ 0.44355535]\n",
      " [ 0.40323213]\n",
      " [ 0.72581784]\n",
      " [ 0.52420177]\n",
      " [ 0.22177767]\n",
      " [ 0.62500981]\n",
      " [ 0.66533302]\n",
      " [ 0.04032321]\n",
      " [ 0.42339374]\n",
      " [ 0.50404017]\n",
      " [ 0.72581784]\n",
      " [ 0.78630266]\n",
      " [ 0.6048482 ]\n",
      " [ 0.6048482 ]\n",
      " [ 0.16129285]\n",
      " [ 0.9072723 ]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 8\n",
      "[[ 0.963962  ]\n",
      " [ 0.12049525]\n",
      " [ 0.14057779]\n",
      " [ 0.57235244]\n",
      " [ 0.44181592]\n",
      " [ 0.4116921 ]\n",
      " [ 0.71293023]\n",
      " [ 0.54222862]\n",
      " [ 0.20082542]\n",
      " [ 0.61251752]\n",
      " [ 0.6526826 ]\n",
      " [ 0.05020635]\n",
      " [ 0.43177464]\n",
      " [ 0.51210481]\n",
      " [ 0.7229715 ]\n",
      " [ 0.78321912]\n",
      " [ 0.61251752]\n",
      " [ 0.61251752]\n",
      " [ 0.16066033]\n",
      " [ 0.92379691]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 9\n",
      "[[ 0.96207557]\n",
      " [ 0.11023783]\n",
      " [ 0.14030269]\n",
      " [ 0.57624318]\n",
      " [ 0.45598374]\n",
      " [ 0.42090806]\n",
      " [ 0.70151344]\n",
      " [ 0.53114589]\n",
      " [ 0.20544322]\n",
      " [ 0.61632966]\n",
      " [ 0.65641615]\n",
      " [ 0.06012972]\n",
      " [ 0.4409513 ]\n",
      " [ 0.52613508]\n",
      " [ 0.72656749]\n",
      " [ 0.77166478]\n",
      " [ 0.62134048]\n",
      " [ 0.61632966]\n",
      " [ 0.16034593]\n",
      " [ 0.9269999 ]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for quantization_level in range(1, 10):\n",
    "    print 'Quantization level: {}'.format(quantization_level)\n",
    "    kwargs = {'quantization_level': quantization_level}\n",
    "    code = encode(test_a, **kwargs)\n",
    "    v = decode(code=code)\n",
    "    print v\n",
    "    print\n",
    "    print\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple demo on a random matrix compression.\n",
    "\n",
    "The precision gets higher as quantization level $s$ becomes larger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
