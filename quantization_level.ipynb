{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quantization Coding\n",
    "\n",
    "Given a matrix $X$ of any shape, the goal is to get a appriximatly good encoder and decoder, such that\n",
    "\n",
    "$$X \\overset{Encoder}{\\longrightarrow\\longrightarrow\\longrightarrow} code \\overset{Decoder}{\\longrightarrow\\longrightarrow\\longrightarrow} X'$$\n",
    "\n",
    "The motivation of quantization is to compress float data (like model gradients) in a lossless way.\n",
    "\n",
    "+ Suppose each value in the array $x \\in X$ is a 8B(64 bit) float\n",
    "+ Want to quantize $X$ with $s$-bit\n",
    "+ $x = sign(x) \\times \\xi(x) \\times \\| X \\|_2^2$, where $\\xi(x)$ is\n",
    "\n",
    "    + Do normalization based on the 2-norm, $x' = \\frac{x}{\\|X\\|_2^2}$\n",
    "    \n",
    "    + Make $2^s$ intervals, corresponding to each value from $[0, 2^s-1]$.\n",
    "    \n",
    "    + For each $x'$, find the interval $[l, l+1]$ where it lies.\n",
    "    \n",
    "      $0 \\le l \\le x' \\le l+1 \\le 2^s -1$\n",
    "    \n",
    "    + Random sample a value in the corersponding interval, $v \\in [l, l+1]$\n",
    "    \n",
    "    + $\\begin{eqnarray}\n",
    "       \\xi(x) =\n",
    "       \\begin{cases}\n",
    "       l, & l \\le v \\le x'\\\\\n",
    "       l+1, & x' \\le v \\le l+1\n",
    "       \\end{cases}\n",
    "      \\end{eqnarray}$\n",
    "+ Therefore compress each float $x$ with 2-bit sign value and $s$-bit $\\xi$. Each 64-bit variable can include $64/(2 + s)$ float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(v, **kwargs):\n",
    "    if isinstance(v, (torch.Tensor, torch.cuda.FloatTensor)):\n",
    "        w = v.cpu().numpy().flat[:]\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        w = v.flat[:]\n",
    "    else:\n",
    "        raise ValueError(\"Object passed to encode not ndarray or torch.Tensor\")\n",
    "\n",
    "    norm = LA.norm(v)\n",
    "\n",
    "    quantization_level = kwargs['quantization_level']\n",
    "    s = (1 << quantization_level) - 1\n",
    "    shape = w.shape\n",
    "    w = np.reshape(w, -1)\n",
    "    num_int_each_64_bits = 64 / (2 + quantization_level)\n",
    "    num_section = num_int_each_64_bits\n",
    "    len_each_section = (w.shape[0] + num_section - 1) / num_section\n",
    "    w = np.pad(w, (0, len_each_section*num_section - w.shape[0]), mode='constant')\n",
    "\n",
    "    sign_array = np.sign(w)\n",
    "    sign_array += 1\n",
    "    sign_array = sign_array.astype('uint64')\n",
    "    normalization_array = np.abs(w) / norm * s\n",
    "\n",
    "    truncated_array = normalization_array.astype(int)\n",
    "    prob_array =  normalization_array - truncated_array\n",
    "    dice_array = np.random.rand(len(prob_array))\n",
    "    xi_array = truncated_array + (dice_array > prob_array)\n",
    "    xi_array = xi_array.astype('uint64')\n",
    "    \n",
    "    old_sign_array = sign_array\n",
    "    old_xi_array = xi_array\n",
    "    \n",
    "    xi_array = xi_array.reshape((num_section, len_each_section))\n",
    "    sign_array = sign_array.reshape((num_section, len_each_section))\n",
    "    \n",
    "    neo_array = np.zeros(len_each_section)\n",
    "    neo_array = neo_array.astype('uint64')\n",
    "\n",
    "    for i in range(num_int_each_64_bits):\n",
    "        xi = xi_array[i]\n",
    "        sign = sign_array[i]\n",
    "        neo_array <<= (2 + quantization_level)\n",
    "        neo_array = neo_array | (sign << quantization_level | xi)\n",
    "\n",
    "    code = {'neo': neo_array, 'norm': norm, 'quantization_level': quantization_level,\n",
    "            'len_each_section': len_each_section, 'num_int_each_64_bits': num_int_each_64_bits,\n",
    "            'shape': shape}\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(code, cuda=False, implementation='numpy', codes=[], **kwargs):\n",
    "    if implementation == 'numpy':\n",
    "        norm = code['norm']\n",
    "        quantization_level = code['quantization_level']\n",
    "        s = (1 << quantization_level) - 1\n",
    "            \n",
    "        real_size = np.prod(code['shape'])\n",
    "        \n",
    "        neo_array = code['neo'].astype('uint64')\n",
    "        num_int_each_64_bits = code['num_int_each_64_bits']\n",
    "        num_section = num_int_each_64_bits\n",
    "        len_each_section = code['len_each_section']\n",
    "        xi_array = np.ones((num_section, len_each_section))\n",
    "        sign_array = np.ones((num_section, len_each_section))\n",
    "        mask_for_xi = (1 << quantization_level) - 1\n",
    "        mask_for_sign = 3 << quantization_level\n",
    "        for i in range(num_int_each_64_bits)[::-1]:\n",
    "            sign_array[i] = (neo_array & mask_for_sign) >> quantization_level\n",
    "            xi_array[i] = neo_array & mask_for_xi\n",
    "            neo_array >>= (2 + quantization_level)\n",
    "        \n",
    "        xi_array = xi_array.reshape(-1).astype('uint64')\n",
    "        sign_array = sign_array.reshape(-1).astype('int8')\n",
    "        sign_array -= 1\n",
    "        v = sign_array * xi_array * norm / s\n",
    "        \n",
    "        v = v[:real_size]\n",
    "        v = v.reshape(code['shape'])\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Whoops, implementation')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each size: 8, total size: 160\n",
      "\n",
      "Original array:\n",
      "[ 0.7083942   0.30100296  0.32128625  0.29623448  0.23249097  0.57685985\n",
      "  0.47257705  0.26852566  0.03364453  0.98916366  0.14209985  0.73961996\n",
      "  0.30185069  0.26503494  0.80978024  0.99585612  0.12184173  0.78061699\n",
      "  0.7229517   0.40223817]\n"
     ]
    }
   ],
   "source": [
    "test_a = np.random.rand(20, 1)\n",
    "\n",
    "print 'each size: {}, total size: {}'.format(test_a[0].nbytes, test_a.nbytes)\n",
    "print\n",
    "print 'Original array:'\n",
    "print test_a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization level: 1\n",
      "[ 2.47587557  2.47587557  2.47587557  2.47587557  2.47587557  2.47587557\n",
      "  0.          2.47587557  2.47587557  2.47587557  2.47587557  2.47587557\n",
      "  2.47587557  2.47587557  0.          2.47587557  2.47587557  0.\n",
      "  2.47587557  2.47587557]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 2\n",
      "[ 0.          0.82529186  0.82529186  0.82529186  0.82529186  0.\n",
      "  0.82529186  0.          0.82529186  0.82529186  0.          0.          0.\n",
      "  0.82529186  0.          1.65058371  0.82529186  0.          0.\n",
      "  0.82529186]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 3\n",
      "[ 1.06108953  0.          0.          0.          0.          0.35369651\n",
      "  0.70739302  0.          0.35369651  0.70739302  0.35369651  1.06108953\n",
      "  0.          0.35369651  0.70739302  1.06108953  0.35369651  1.06108953\n",
      "  1.06108953  0.35369651]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 4\n",
      "[ 0.66023349  0.16505837  0.16505837  0.33011674  0.33011674  0.66023349\n",
      "  0.33011674  0.33011674  0.          0.82529186  0.          0.66023349\n",
      "  0.16505837  0.33011674  0.66023349  1.1554086   0.          0.66023349\n",
      "  0.82529186  0.33011674]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 5\n",
      "[ 0.63893563  0.23960086  0.39933477  0.23960086  0.15973391  0.55906868\n",
      "  0.47920172  0.31946782  0.07986695  1.0382704   0.07986695  0.79866954\n",
      "  0.23960086  0.31946782  0.87853649  0.95840345  0.15973391  0.71880258\n",
      "  0.79866954  0.47920172]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 6\n",
      "[ 0.74669263  0.3143969   0.35369651  0.27509729  0.19649806  0.58949418\n",
      "  0.51089496  0.27509729  0.          1.02178992  0.15719845  0.74669263\n",
      "  0.27509729  0.23579767  0.82529186  1.02178992  0.15719845  0.74669263\n",
      "  0.70739302  0.39299612]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 7\n",
      "[ 0.72131808  0.29242625  0.31192133  0.31192133  0.233941    0.5848525\n",
      "  0.48737708  0.25343608  0.03899017  0.97475416  0.13646558  0.72131808\n",
      "  0.29242625  0.27293117  0.79929841  1.01374433  0.1169705   0.79929841\n",
      "  0.74081316  0.38990166]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 8\n",
      "[ 0.69907075  0.31069811  0.33011674  0.30098879  0.22331427  0.58255896\n",
      "  0.46604717  0.27186085  0.03883726  0.98064091  0.13593042  0.74761733\n",
      "  0.30098879  0.27186085  0.81558254  0.99035023  0.11651179  0.78645459\n",
      "  0.7281987   0.39808195]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 9\n",
      "[ 0.70739302  0.30524493  0.32462556  0.29555462  0.22772241  0.58141892\n",
      "  0.47482545  0.27132883  0.02907095  0.99325732  0.14050957  0.74130912\n",
      "  0.30524493  0.26648367  0.81398649  0.99325732  0.1259741   0.78491554\n",
      "  0.72192849  0.40699324]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for quantization_level in range(1, 10):\n",
    "    print 'Quantization level: {}'.format(quantization_level)\n",
    "    kwargs = {'quantization_level': quantization_level}\n",
    "    code = encode(test_a, **kwargs)\n",
    "    v = decode(code=code)\n",
    "    print v\n",
    "    print\n",
    "    print\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple demo on a random matrix compression.\n",
    "\n",
    "The precision gets higher as quantization level $s$ becomes larger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
