{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Apply Bucketing\n",
    "\n",
    "In vanilla quantization, normalization may introduce high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(v, bucket_size=512, **kwargs):\n",
    "    if isinstance(v, (torch.Tensor, torch.cuda.FloatTensor)):\n",
    "        w = v.cpu().numpy().flat[:]\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        w = v.flat[:]\n",
    "    else:\n",
    "        raise ValueError(\"Object passed to encode not ndarray or torch.Tensor\")\n",
    "        \n",
    "    # Apply bucketing\n",
    "    if bucket_size != 0:\n",
    "        code_buckets = []\n",
    "        shape = v.shape\n",
    "        print 'shape\\t', shape\n",
    "        \n",
    "        buckets = np.split(w, (w.shape[0]+bucket_size-1) / bucket_size)\n",
    "        for bucket in buckets:\n",
    "            code = encode(bucket, bucket_size=0, **kwargs)\n",
    "            code_buckets.append(code)\n",
    "        return {'code_buckets': code_buckets, 'shape': shape}\n",
    "\n",
    "    norm = LA.norm(v)\n",
    "\n",
    "    quantization_level = kwargs['quantization_level']\n",
    "    s = (1 << quantization_level) - 1\n",
    "    shape = v.shape\n",
    "    num_int_each_64_bits = 64 / (2 + quantization_level)\n",
    "    num_section = num_int_each_64_bits\n",
    "    len_each_section = (w.shape[0] + num_section - 1) / num_section\n",
    "    w = np.pad(w, (0, len_each_section*num_section - w.shape[0]), mode='constant')\n",
    "\n",
    "    sign_array = np.sign(w)\n",
    "    sign_array += 1\n",
    "    sign_array = sign_array.astype('uint64')\n",
    "    normalization_array = np.abs(w) / norm * s\n",
    "\n",
    "    truncated_array = normalization_array.astype(int)\n",
    "    prob_array =  normalization_array - truncated_array\n",
    "    dice_array = np.random.rand(len(prob_array))\n",
    "    xi_array = truncated_array + (dice_array > prob_array)\n",
    "    xi_array = xi_array.astype('uint64')\n",
    "    \n",
    "    old_sign_array = sign_array\n",
    "    old_xi_array = xi_array\n",
    "    \n",
    "    xi_array = xi_array.reshape((num_section, len_each_section))\n",
    "    sign_array = sign_array.reshape((num_section, len_each_section))\n",
    "    \n",
    "    neo_array = np.zeros(len_each_section)\n",
    "    neo_array = neo_array.astype('uint64')\n",
    "\n",
    "    for i in range(num_int_each_64_bits):\n",
    "        xi = xi_array[i]\n",
    "        sign = sign_array[i]\n",
    "        neo_array <<= (2 + quantization_level)\n",
    "        neo_array = neo_array | (sign << quantization_level | xi)\n",
    "\n",
    "    code = {'neo': neo_array, 'norm': norm, 'quantization_level': quantization_level,\n",
    "            'len_each_section': len_each_section, 'num_int_each_64_bits': num_int_each_64_bits,\n",
    "            'shape': shape}\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(code, bucket_size=512, cuda=False, implementation='numpy', codes=[], **kwargs):\n",
    "    if implementation == 'numpy':\n",
    "        # Decode from bucketing\n",
    "        if bucket_size != 0:\n",
    "            v_list = []\n",
    "            for code_bucket in code['code_buckets']:\n",
    "                v = decode(code=code_bucket, bucket_size=0,\n",
    "                           cuda=cuda, implementation=implementation,\n",
    "                           codes=codes, **kwargs)\n",
    "                v_list.extend(v)\n",
    "            v = np.array(v_list)\n",
    "            v = v.reshape(code['shape'])\n",
    "        else:\n",
    "            norm = code['norm']\n",
    "            quantization_level = code['quantization_level']\n",
    "            s = (1 << quantization_level) - 1\n",
    "\n",
    "            real_size = np.prod(code['shape'])\n",
    "\n",
    "            neo_array = code['neo'].astype('uint64')\n",
    "            num_int_each_64_bits = code['num_int_each_64_bits']\n",
    "            num_section = num_int_each_64_bits\n",
    "            len_each_section = code['len_each_section']\n",
    "            xi_array = np.ones((num_section, len_each_section))\n",
    "            sign_array = np.ones((num_section, len_each_section))\n",
    "            mask_for_xi = (1 << quantization_level) - 1\n",
    "            mask_for_sign = 3 << quantization_level\n",
    "            for i in range(num_int_each_64_bits)[::-1]:\n",
    "                sign_array[i] = (neo_array & mask_for_sign) >> quantization_level\n",
    "                xi_array[i] = neo_array & mask_for_xi\n",
    "                neo_array >>= (2 + quantization_level)\n",
    "\n",
    "            xi_array = xi_array.reshape(-1).astype('uint64')\n",
    "            sign_array = sign_array.reshape(-1).astype('int8')\n",
    "            sign_array -= 1\n",
    "            v = sign_array * xi_array * norm / s\n",
    "\n",
    "            v = v[:real_size]\n",
    "            v = v.reshape(code['shape'])\n",
    "    else:\n",
    "        raise ValueError('Whoops, implementation')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each size: 32, total size: 160\n",
      "\n",
      "Original array:\n",
      "[[ 0.61673933  0.86467965  0.25606733  0.35983486]\n",
      " [ 0.8342569   0.04360524  0.5447879   0.77115447]\n",
      " [ 0.78620767  0.27697439  0.03115012  0.92166798]\n",
      " [ 0.7214374   0.56996659  0.00916378  0.02055269]\n",
      " [ 0.06300408  0.12190967  0.82373013  0.13947622]]\n"
     ]
    }
   ],
   "source": [
    "test_a = np.random.rand(5, 4)\n",
    "\n",
    "print 'each size: {}, total size: {}'.format(test_a[0].nbytes, test_a.nbytes)\n",
    "print\n",
    "print 'Original array:'\n",
    "print test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization level: 1\n",
      "shape\t(5, 4)\n",
      "[[ 0.          1.4209414   0.          1.4209414 ]\n",
      " [ 1.4209414   1.26024276  0.          0.        ]\n",
      " [ 0.          1.26024276  1.3022519   0.        ]\n",
      " [ 0.          1.3022519   1.3022519   0.84689954]\n",
      " [ 0.          0.84689954  0.          0.84689954]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 2\n",
      "shape\t(5, 4)\n",
      "[[ 0.47364713  0.47364713  0.47364713  0.        ]\n",
      " [ 0.94729427  0.42008092  0.84016184  0.42008092]\n",
      " [ 0.42008092  0.          0.43408397  1.3022519 ]\n",
      " [ 0.86816793  0.86816793  0.43408397  0.28229985]\n",
      " [ 0.28229985  0.28229985  0.56459969  0.28229985]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 3\n",
      "shape\t(5, 4)\n",
      "[[ 0.81196651  1.01495814  0.20299163  0.20299163]\n",
      " [ 1.01495814  0.18003468  0.72013872  0.72013872]\n",
      " [ 0.9001734   0.36006936  0.18603599  0.74414394]\n",
      " [ 0.55810796  0.74414394  0.18603599  0.        ]\n",
      " [ 0.12098565  0.2419713   0.72591389  0.2419713 ]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 4\n",
      "shape\t(5, 4)\n",
      "[[ 0.56837656  0.94729427  0.18945885  0.28418828]\n",
      " [ 0.75783541  0.          0.58811329  0.84016184]\n",
      " [ 0.84016184  0.33606473  0.08681679  0.86816793]\n",
      " [ 0.78135114  0.52090076  0.08681679  0.05645997]\n",
      " [ 0.11291994  0.11291994  0.79043957  0.16937991]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 5\n",
      "shape\t(5, 4)\n",
      "[[ 0.59587865  0.82506275  0.2291841   0.32085774]\n",
      " [ 0.87089957  0.08130598  0.56914189  0.73175386]\n",
      " [ 0.77240685  0.24391795  0.          0.88217064]\n",
      " [ 0.75614626  0.58811376  0.04200813  0.        ]\n",
      " [ 0.05463868  0.1365967   0.84689954  0.16391604]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 6\n",
      "shape\t(5, 4)\n",
      "[[ 0.63152951  0.87963039  0.24810088  0.33831938]\n",
      " [ 0.81196651  0.06001156  0.56010789  0.76014642]\n",
      " [ 0.80015413  0.26005009  0.04134133  0.90950926]\n",
      " [ 0.72347328  0.55810796  0.02067067  0.0268857 ]\n",
      " [ 0.06721425  0.1344285   0.83345669  0.14787135]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 7\n",
      "shape\t(5, 4)\n",
      "[[ 0.62655684  0.87270417  0.24614733  0.36922099]\n",
      " [ 0.82795011  0.03969269  0.53585125  0.76408419]\n",
      " [ 0.7938537   0.26792563  0.04101581  0.91260172]\n",
      " [ 0.72803059  0.56396736  0.          0.026674  ]\n",
      " [ 0.066685    0.12670151  0.82689404  0.14003851]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 8\n",
      "shape\t(5, 4)\n",
      "[[ 0.61295511  0.8692818   0.25075436  0.35662843]\n",
      " [ 0.83027556  0.03953703  0.54363413  0.77591417]\n",
      " [ 0.79074055  0.28170132  0.03574809  0.9243435 ]\n",
      " [ 0.72517557  0.56686259  0.00510687  0.02324822]\n",
      " [ 0.06310232  0.11956229  0.82697249  0.13616816]]\n",
      "\n",
      "\n",
      "\n",
      "Quantization level: 9\n",
      "shape\t(5, 4)\n",
      "[[ 0.6145363   0.86201924  0.25860577  0.36149194]\n",
      " [ 0.83699288  0.04439211  0.54257027  0.76946329]\n",
      " [ 0.78426066  0.27868382  0.0331297   0.91998617]\n",
      " [ 0.72375644  0.57085015  0.00764531  0.02154539]\n",
      " [ 0.06463617  0.12264299  0.82535415  0.1408737 ]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket_size = 6\n",
    "for quantization_level in range(1, 10):\n",
    "    print 'Quantization level: {}'.format(quantization_level)\n",
    "    kwargs = {'quantization_level': quantization_level}\n",
    "    code = encode(test_a, bucket_size=bucket_size, **kwargs)\n",
    "    v = decode(code=code, bucket_size=bucket_size)\n",
    "    print v\n",
    "    print\n",
    "    print\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
